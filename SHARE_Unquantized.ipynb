{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install semantichar==0.2\n",
    "pip install pillow==9.2.0\n",
    "pip install ftfy\n",
    "pip install einops iopath regex wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/xiyuanzh/SHARE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ./SHARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir model\n",
    "mkdir dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ./easy_imu_phone.zip -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/keerthiv/HAR_models/share/SHARE/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keerthiv/.local/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd /home/keerthiv/HAR_models/share/SHARE/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from semantichar.utils import all_label_augmentation\n",
    "\n",
    "def DataBatch(data, label, text, l, batchsize, shuffle=True):\n",
    "    \n",
    "    n = data.shape[0]\n",
    "    print( n )\n",
    "    if shuffle:\n",
    "        index = np.random.permutation(n)\n",
    "    else:\n",
    "        index = np.arange(n)\n",
    "    for i in range(int(np.ceil(n/batchsize))):\n",
    "        inds = index[i*batchsize : min(n,(i+1)*batchsize)]\n",
    "        yield inds, data[inds], label[inds], text[inds], l[inds]\n",
    "        \n",
    "def trainer(opt, \n",
    "            enc, \n",
    "            dec, \n",
    "            cross_entropy, \n",
    "            optimizer, \n",
    "            tr_data, \n",
    "            tr_label, \n",
    "            tr_text, \n",
    "            len_text, \n",
    "            break_step, \n",
    "            vocab_size, \n",
    "            device\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    Args:\n",
    "        opt: user-specified configurations.\n",
    "        enc: encoder of the model.\n",
    "        dec: decoder of the model.\n",
    "        cross_entropy: loss function.\n",
    "        optimizer: optimizer (default is Adam).\n",
    "        tr_data, tr_label, tr_text, len_text: training data, label, label sequence, length of the label sequence. \n",
    "        break_step: length of the longest label sequence length (i.e., maximum decoding step).\n",
    "        vocab_size: label name vocabulary size.\n",
    "        device: cuda or cpu.\n",
    "    \"\"\"\n",
    "\n",
    "    enc.train()\n",
    "    dec.train()  \n",
    "\n",
    "    total_loss = 0\n",
    "    for inds,batch_data, batch_label, batch_text, batch_len in \\\n",
    "        DataBatch(tr_data, tr_label, tr_text, len_text, opt['batchSize'],shuffle=True):\n",
    "        \n",
    "        batch_text = all_label_augmentation(batch_text, opt['prob'], break_step, vocab_size)\n",
    "\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "        batch_text = batch_text.to(device)\n",
    "        batch_len = batch_len.to(device)\n",
    "\n",
    "        enc_hidden = enc(batch_data)\n",
    "        pred, batch_text_sorted, decode_lengths, sort_ind \\\n",
    "            = dec(enc_hidden, batch_text, batch_len)\n",
    "        \n",
    "        targets = batch_text_sorted[:, 1:]\n",
    "\n",
    "        pred, *_ = pack_padded_sequence(pred, decode_lengths, batch_first=True)\n",
    "        targets, *_ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "\n",
    "        loss = cross_entropy(pred, targets.long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += len(batch_data) * loss.item()\n",
    "\n",
    "    total_loss /= len(tr_data)\n",
    "    \n",
    "    return total_loss \n",
    "\n",
    "  \n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "import os\n",
    "\n",
    "def evaluate(opt, \n",
    "             enc, \n",
    "             dec, \n",
    "             test_data, \n",
    "             test_label, \n",
    "             test_text, \n",
    "             test_len_text, \n",
    "             pred_dict, \n",
    "             seqs, \n",
    "             break_step, \n",
    "             class_num, \n",
    "             vocab_size, \n",
    "             device,\n",
    "             load=True):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    Args:\n",
    "        opt: user-specified configurations.\n",
    "        enc: encoder of the model.\n",
    "        dec: decoder of the model.\n",
    "        test_data, test_label, test_text, test_len_text: test data, label, label sequence, length of the label sequence.\n",
    "        pred_dict: mapping from label token-id sequence to label id.\n",
    "        seqs: label token-id sequence for all classes.\n",
    "        break_step: length of the longest label sequence length (i.e., maximum decoding step).\n",
    "        class_num: number of classes.\n",
    "        vocab_size: label name vocabulary size.\n",
    "        device: cuda or cpu.\n",
    "        load: load saved model weights or not.\n",
    "    \"\"\"\n",
    "\n",
    "    #enc.eval()\n",
    "    #dec.eval()\n",
    "    #enc.cpu()\n",
    "    #dec.cpu()\n",
    "    #enc.eval()\n",
    "    #quantized_enc = torch.quantization.convert(enc, inplace=False)\n",
    "    #print(\"enc Model converted to quantized version\")\n",
    "\n",
    "\n",
    "    #dec.eval()\n",
    "    #quantized_dec = torch.quantization.convert(dec, inplace=False)\n",
    "    #print(\"dec Model converted to quantized version\")\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    #print( opt )\n",
    "    #print(enc.weight.device)  # Should print 'cuda:0' if on GPU\n",
    "    #print(dec.weight.device)  # Should print 'cuda:0' if on GPU\n",
    "    enc.to(device)\n",
    "    dec.to(device)\n",
    "    print(next(enc.parameters()).device)\n",
    "    print(next(dec.parameters()).device)\n",
    "    print(enc.layer1.weight.device)\n",
    "    #quantized_enc.eval()\n",
    "    #quantized_dec.eval()\n",
    "    #quantized_enc.to(device)\n",
    "    #quantized_dec.to(device)\n",
    "    \n",
    "    if load:\n",
    "        enc = torch.load(os.path.join('model', f\"{opt['run_tag']}_enc.pth\"))\n",
    "        dec = torch.load(os.path.join('model', f\"{opt['run_tag']}_dec.pth\"))\n",
    "        #enc.load_state_dict(torch.load(opt['model_path'] + opt['run_tag'] + '_enc.pth', map_location=device, weights_only=False))\n",
    "        #dec.load_state_dict(torch.load(opt['model_path'] + opt['run_tag'] + '_dec.pth', map_location=device, weights_only=False))\n",
    "\n",
    "    hypotheses = list()\n",
    "    batch_size = test_data.size(0)\n",
    "    pred_whole = torch.zeros_like(test_label)\n",
    "    seqs = seqs.to(device)\n",
    "\n",
    "    total_evaluation_time = 0  # Initialize total evaluation time\n",
    "    total_samples = 0  # Initialize total number of samples\n",
    "\n",
    "    for batch_idx, (batch_data, batch_label, batch_text, batch_len) in enumerate(\n",
    "        DataBatch(test_data, test_label, test_text, test_len_text, opt['batchSize'], shuffle=False)\n",
    "    ):\n",
    "\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "        batch_text = batch_text.to(device)\n",
    "        batch_len = batch_len.to(device)\n",
    "        batch_data = batch_data.to_mkldnn()\n",
    "        enc = enc.to_mkldnn()\n",
    "        # Start timing after sending to device\n",
    "        \n",
    "\n",
    "        batch_size = batch_data.size(0)\n",
    "        total_samples += batch_size  # Accumulate the number of samples\n",
    "        start_time = time.time()\n",
    "        encoder_out = enc(batch_data)  # (batch_size, enc_seq_len, encoder_dim)\n",
    "        enc_seq_len = encoder_out.size(1)\n",
    "        encoder_dim = encoder_out.size(2)\n",
    "\n",
    "        encoder_out = encoder_out.unsqueeze(1).expand(batch_size, class_num, enc_seq_len, encoder_dim)\n",
    "        encoder_out = encoder_out.reshape(batch_size * class_num, enc_seq_len, encoder_dim)\n",
    "\n",
    "        k_prev_words = seqs[:, 0].unsqueeze(0).expand(batch_size, class_num).long()  # (batch_size, class_num)\n",
    "        k_prev_words = k_prev_words.reshape(batch_size * class_num, 1)  # (batch_size * class_num, 1)\n",
    "\n",
    "        h, c = dec.init_hidden_state(encoder_out)\n",
    "\n",
    "        seq_scores = torch.zeros((batch_size, class_num)).to(device)\n",
    "\n",
    "        for step in range(1, break_step):\n",
    "            embeddings = dec.embedding(k_prev_words).squeeze(1)  # (batch_size * class_num, embed_dim)\n",
    "            h, c = dec.decode_step(embeddings, (h, c))\n",
    "            scores = dec.fc(h.reshape(batch_size, class_num, -1))  # (batch_size, class_num, vocab_size)\n",
    "            scores = F.log_softmax(scores, dim=-1)\n",
    "            k_prev_words = seqs[:, step].unsqueeze(0).expand(batch_size, class_num).long()\n",
    "            for batch_i in range(batch_size):\n",
    "                for class_i in range(class_num):\n",
    "                    if k_prev_words[batch_i, class_i] != 0:\n",
    "                        seq_scores[batch_i, class_i] += scores[batch_i, class_i, k_prev_words[batch_i, class_i]]\n",
    "            k_prev_words = k_prev_words.reshape(batch_size * class_num, 1)  # (batch_size * class_num, 1)\n",
    "\n",
    "        max_indices = seq_scores.argmax(dim=1)\n",
    "        for batch_i in range(batch_size):\n",
    "            max_i = max_indices[batch_i]\n",
    "            seq = seqs[max_i].tolist()\n",
    "            hypotheses.append([w for w in seq if w not in {0, vocab_size - 1}])\n",
    "            pred_whole[batch_i + batch_idx * opt['batchSize']] = pred_dict[\"#\".join(map(str, hypotheses[-1]))]\n",
    "\n",
    "        # End timing for the batch\n",
    "        end_time = time.time()\n",
    "        batch_evaluation_time = end_time - start_time  # Calculate batch evaluation time\n",
    "        total_evaluation_time += batch_evaluation_time  # Accumulate total evaluation time\n",
    "\n",
    "        print(f'Batch {batch_idx + 1} Evaluation Time: {batch_evaluation_time:.2f} seconds')\n",
    "\n",
    "    acc = accuracy_score(test_label.cpu().numpy(), pred_whole.cpu().numpy())\n",
    "    prec = precision_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
    "    rec = recall_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
    "    f1 = f1_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
    "\n",
    "    print(f'Total Evaluation Time: {total_evaluation_time:.2f} seconds')\n",
    "    \n",
    "    # Calculate evaluation time per batch and per sample\n",
    "    eval_time_per_batch = total_evaluation_time / (batch_idx + 1)\n",
    "    eval_time_per_sample = total_evaluation_time / total_samples\n",
    "\n",
    "    print(f'Average Evaluation Time per Batch: {eval_time_per_batch:.2f} seconds')\n",
    "    print(f'Average Evaluation Time per Sample: {eval_time_per_sample:.6f} seconds')\n",
    "\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.quantization\n",
    "\n",
    "import semantichar.data \n",
    "from semantichar import imagebind_model\n",
    "from semantichar.imagebind_model import ModalityType\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "import torch.quantization as quant\n",
    "class CustomBatchNorm1d(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CustomBatchNorm1d, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert batch normalization to quantization-aware operations\n",
    "        if self.training:\n",
    "            x = self.bn(x)\n",
    "        else:\n",
    "            x = torch.nn.functional.batch_norm(\n",
    "                x, \n",
    "                self.bn.running_mean, \n",
    "                self.bn.running_var, \n",
    "                self.bn.weight, \n",
    "                self.bn.bias, \n",
    "                training=False\n",
    "            )\n",
    "        return x\n",
    "    \n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size, bias=True):\n",
    "        super(QuantizedLinear, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size, bias=bias)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)  # Quantize input\n",
    "        x = self.fc(x)     # Apply linear transformation\n",
    "        x = self.dequant(x)  # Dequantize output\n",
    "        return x\n",
    "\n",
    "    def fuse_model(self):\n",
    "        # Fuse the linear layer with the quantization stubs\n",
    "        torch.quantization.fuse_modules(self, ['quant', 'fc', 'dequant'], inplace=True)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.fc.weight\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self.fc.bias\n",
    " \n",
    "    \n",
    "class CustomObserver(quant.MinMaxObserver):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomObserver, self).__init__(*args, **kwargs)\n",
    "\n",
    "class QuantizedConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n",
    "        super(QuantizedConv1d, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.weight_fake_quant = quant.FakeQuantize.with_args(observer=quant.default_weight_observer, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine)()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)  # Quantize input\n",
    "        x = self.conv(x)   # Apply convolution\n",
    "        x = self.bn(x)     # Apply batch normalization\n",
    "        x = self.relu(x)   # Apply ReLU\n",
    "        x = self.dequant(x)  # Dequantize output\n",
    "        return x\n",
    "\n",
    "    def fuse_model(self):\n",
    "        # Fuse the conv, batch norm, and ReLU layers\n",
    "        torch.quantization.fuse_modules(self, [['conv', 'bn', 'relu']], inplace=True)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.conv.weight\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self.conv.bias\n",
    "\n",
    "    def apply_weight_fake_quant(self):\n",
    "        self.weight_fake_quant(self.conv.weight)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_input: int,\n",
    "                 d_model: int,\n",
    "                 d_output: int,\n",
    "                 seq_len: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Conv1d(in_channels=d_input, out_channels=d_model, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(d_model)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.layer2 = nn.Conv1d(in_channels=d_model, out_channels=d_output, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(d_model)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b,t,c = x.size()\n",
    "\n",
    "        out = self.layer1(x.permute(0,2,1))\n",
    "        #out = self.act1((out))\n",
    "        out = self.act1(self.bn1(out))\n",
    "\n",
    "        out = self.layer2(out)\n",
    "        #out = self.act2(out)\n",
    "        out = self.act2(self.bn2(out)) # (b, d_output, seq_len)\n",
    "\n",
    "        return out.permute(0,2,1) # (b, seq_len, d_output)\n",
    "        \n",
    "class Encoder_q(nn.Module):\n",
    "    def __init__(self, d_input: int, d_model: int, d_output: int, seq_len: int):\n",
    "        super().__init__()\n",
    "        self.layer1 = QuantizedConv1d(in_channels=d_input, out_channels=d_model, kernel_size=3, padding=1)\n",
    "        self.layer2 = QuantizedConv1d(in_channels=d_model, out_channels=d_output, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, t, c = x.size()\n",
    "        out = self.layer1(x.permute(0, 2, 1))\n",
    "        self.layer1.apply_weight_fake_quant()  # Apply weight fake quantization for layer1\n",
    "        out = self.layer2(out)\n",
    "        self.layer2.apply_weight_fake_quant()  # Apply weight fake quantization for layer2\n",
    "        out = out.permute(0, 2, 1)  # (b, seq_len, d_output)\n",
    "        return out\n",
    "\n",
    "class QuantizedLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(QuantizedLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.ih = nn.Linear(input_size, 4 * hidden_size, bias=bias)\n",
    "        self.hh = nn.Linear(hidden_size, 4 * hidden_size, bias=bias)\n",
    "\n",
    "        self.quant1 = QuantStub()\n",
    "        self.dequant1 = DeQuantStub()\n",
    "\n",
    "        self.quant2 = QuantStub()\n",
    "        self.dequant2 = DeQuantStub()\n",
    "\n",
    "        self.quant3 = QuantStub()\n",
    "        self.dequant3 = DeQuantStub()\n",
    "\n",
    "\n",
    "    def forward(self, input, hx):\n",
    "        hx, cx = hx\n",
    "\n",
    "        # Quantize inputs\n",
    "        #input = self.quant1(input)\n",
    "        hx = self.quant2(hx)\n",
    "        cx = self.quant3(cx)\n",
    "\n",
    "        # LSTM cell operations\n",
    "        ih_out = self.ih(input)\n",
    "        hh_out = self.hh(hx)\n",
    "\n",
    "        # Dequantize before addition and multiplication\n",
    "        #ih_out = self.dequant1(ih_out)\n",
    "        hh_out = self.dequant2(hh_out)\n",
    "        cx = self.dequant3(cx)\n",
    "\n",
    "        gates = ih_out + hh_out\n",
    "\n",
    "        i, f, g, o = gates.chunk(4, 1)\n",
    "\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        g = torch.tanh(g)\n",
    "        o = torch.sigmoid(o)\n",
    "\n",
    "        cy = f * cx + i * g\n",
    "        hy = o * torch.tanh(cy)\n",
    "\n",
    "        return hy, cy\n",
    "\n",
    "    def _init_hidden(self, batch_size, device):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(batch_size, self.hidden_size).zero_().to(device),\n",
    "                weight.new(batch_size, self.hidden_size).zero_().to(device))\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, decoder_dim, vocab, encoder_dim, device, dropout=0.5):\n",
    "      \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embed_dim)  \n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim, decoder_dim, bias=True)  \n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.fc = nn.Linear(decoder_dim, self.vocab_size) \n",
    "        self.load_pretrained_embeddings()\n",
    "\n",
    "    def load_pretrained_embeddings(self):\n",
    "\n",
    "        inputs = {\n",
    "            ModalityType.TEXT: semantichar.data.load_and_transform_text(self.vocab, self.device)\n",
    "        }\n",
    "        model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "        model.eval()\n",
    "        model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(inputs)['text']\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \n",
    "        mean_encoder_out = encoder_out.mean(dim=1) \n",
    "        h = self.init_h(mean_encoder_out) \n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  \n",
    "        seq_len = encoder_out.size(1)\n",
    "\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        embeddings = self.embedding(encoded_captions.long()) \n",
    "\n",
    "        h, c = self.init_hidden_state(encoder_out)  \n",
    "\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        \n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n",
    "\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths]) \n",
    "            h, c = self.decode_step(embeddings[:batch_size_t, t, :], \\\n",
    "                (h[:batch_size_t], c[:batch_size_t]))\n",
    "            preds = self.fc(self.dropout(h))  \n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "        return predictions, encoded_captions, decode_lengths, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.quantization as quant\n",
    "from torch.quantization.observer import MovingAverageMinMaxObserver, default_weight_observer\n",
    "\n",
    "#from semantichar.seq2seq import Encoder, Decoder\n",
    "#from semantichar.exp import trainer, evaluate\n",
    "from semantichar.dataset import prepare_dataset\n",
    "\n",
    "class CustomObserver(MovingAverageMinMaxObserver):\n",
    "    def calculate_qparams(self):\n",
    "        scale, _ = super().calculate_qparams()\n",
    "        zero_point = torch.tensor(0, dtype=torch.int32)\n",
    "        return scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/keerthiv/HAR_models/share/SHARE\n"
     ]
    }
   ],
   "source": [
    "cd /home/keerthiv/HAR_models/share/SHARE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: easy_imu_phone\n",
      "data_path: ./\n",
      "manualSeed: 2023\n",
      "epochs: 150\n",
      "early_stopping: 50\n",
      "batchSize: 16\n",
      "lr: 0.0001\n",
      "prob: 0.4\n",
      "cuda: True\n",
      "run_tag: test\n",
      "model_path: ./model/\n",
      "Random Seed:  2023\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "# Updated values for the arguments\n",
    "dataset = 'easy_imu_phone'\n",
    "data_path = './'\n",
    "manualSeed = 2023\n",
    "epochs = 150\n",
    "early_stopping = 50\n",
    "batchSize = 16\n",
    "lr = 1e-4\n",
    "prob = 0.4\n",
    "cuda = True\n",
    "run_tag = 'test'\n",
    "model_path = './model/'\n",
    "\n",
    "# Print the updated values\n",
    "print(f'dataset: {dataset}')\n",
    "print(f'data_path: {data_path}')\n",
    "print(f'manualSeed: {manualSeed}')\n",
    "print(f'epochs: {epochs}')\n",
    "print(f'early_stopping: {early_stopping}')\n",
    "print(f'batchSize: {batchSize}')\n",
    "print(f'lr: {lr}')\n",
    "print(f'prob: {prob}')\n",
    "print(f'cuda: {cuda}')\n",
    "print(f'run_tag: {run_tag}')\n",
    "print(f'model_path: {model_path}')\n",
    "\n",
    "# Set random seed\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not cuda:\n",
    "    print(\"You have a cuda device, so you might want to run with --cuda as option\")\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "\n",
    "data_root = data_path + '/dataset/' + dataset\n",
    "config_file = data_path + '/configs/' + dataset + '.json'\n",
    "with open(config_file, 'r') as config_file:\n",
    "    data = json.load(config_file)\n",
    "    label_dictionary = {int(k): v for k, v in data['label_dictionary'].items()}\n",
    "\n",
    "tr_data = np.load(data_root + '/x_train.npy')\n",
    "tr_label = np.load(data_root + '/y_train.npy')\n",
    "\n",
    "test_data = np.load(data_root + '/x_test.npy')\n",
    "test_label = np.load(data_root + '/y_test.npy')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "{0: ['start', 'standing', 'end'], 1: ['start', 'walking', 'end'], 2: ['start', 'turning', 'left', 'end'], 3: ['start', 'sitting', 'end'], 4: ['start', 'turning', 'right', 'end'], 5: ['start', 'waving', 'right', 'hand', 'end'], 6: ['start', 'waving', 'left', 'hand', 'end'], 7: ['start', 'jumping', 'end'], 8: ['start', 'lying', 'end'], 9: ['start', 'drinking', 'water', 'end'], 10: ['start', 'throwing', 'end'], 11: ['start', 'kicking', 'right', 'foot', 'end'], 12: ['start', 'kicking', 'left', 'foot', 'end'], 13: ['start', 'golf', 'swinging', 'end'], 14: ['start', 'basketball', 'shooting', 'end'], 15: ['start', 'boxing', 'end'], 16: ['start', 'torso', 'twisting', 'end'], 17: ['start', 'squatting', 'end'], 18: ['start', 'forward', 'bending', 'in', 'standing', 'position', 'end'], 19: ['start', 'forward', 'bending', 'in', 'sitting', 'position', 'end'], 20: ['start', 'leg', 'stretching', 'end'], 21: ['start', 'pushing', 'end'], 22: ['start', 'pulling', 'end'], 23: ['start', 'right', 'hand', 'up', 'end'], 24: ['start', 'left', 'hand', 'up', 'end'], 25: ['start', 'drawing', 'clockwise', 'end'], 26: ['start', 'drawing', 'counter', 'clockwise', 'end']}\n",
      "load dataset\n",
      "torch.Size([4953, 150, 9]) torch.Size([1320, 150, 9])\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(3)\n",
      "tensor(4)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(5)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(3)\n",
      "tensor(6)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(7)\n",
      "tensor(6)\n",
      "tensor(8)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(7)\n",
      "tensor(4)\n",
      "tensor(8)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(9)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(10)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(11)\n",
      "tensor(12)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(13)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(14)\n",
      "tensor(6)\n",
      "tensor(15)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(14)\n",
      "tensor(4)\n",
      "tensor(15)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(16)\n",
      "tensor(17)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(18)\n",
      "tensor(19)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(20)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(21)\n",
      "tensor(22)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(23)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(24)\n",
      "tensor(25)\n",
      "tensor(26)\n",
      "tensor(1)\n",
      "tensor(27)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(24)\n",
      "tensor(25)\n",
      "tensor(26)\n",
      "tensor(5)\n",
      "tensor(27)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(28)\n",
      "tensor(29)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(30)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(31)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(6)\n",
      "tensor(8)\n",
      "tensor(32)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(4)\n",
      "tensor(8)\n",
      "tensor(32)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(33)\n",
      "tensor(34)\n",
      "tensor(36)\n",
      "tensor(0)\n",
      "tensor(33)\n",
      "tensor(35)\n",
      "tensor(34)\n",
      "tensor(36)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "seq_len, dim, class_num, vocab_size, break_step, word_list, pred_dict, seqs, \\\n",
    "    tr_data, test_data, \\\n",
    "    tr_label, test_label, \\\n",
    "    tr_text, test_text, \\\n",
    "    len_text, test_len_text = prepare_dataset(tr_data, tr_label, test_data, test_label, label_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomObserver(MovingAverageMinMaxObserver):\n",
    "    def calculate_qparams(self):\n",
    "        scale, _ = super().calculate_qparams()\n",
    "        zero_point = torch.tensor(0, dtype=torch.int32)\n",
    "        return scale, zero_point\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "enc = Encoder(d_input=dim, d_model=128, d_output=128, seq_len=seq_len).to(device)\n",
    "dec = Decoder(embed_dim=1024, decoder_dim=128, vocab=word_list, encoder_dim=128, device=device).to(device)\n",
    "\n",
    "# Apply the quantization configuration to the embedding layers in the decoder\n",
    "enc.train()\n",
    "dec.train()\n",
    "\n",
    "# Move the models to GPU for training\n",
    "enc.to(device)\n",
    "dec.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "params = list(enc.parameters()) + list(dec.parameters())\n",
    "optimizer = optim.Adam(params, lr=1e-4)\n",
    "cross_entropy = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batchSize': batchSize,\n",
    "    'epochs': epochs,\n",
    "    'run_tag': run_tag,\n",
    "    'dataset': dataset,\n",
    "    'cuda': cuda,\n",
    "    'manualSeed': manualSeed,\n",
    "    'data_path': data_path,\n",
    "    'early_stopping': early_stopping,\n",
    "    'lr': 0.0001,\n",
    "    'prob': prob,\n",
    "    'model_path': model_path\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1, 36,  0,  0,  0,  0],\n",
      "        [ 0,  2, 36,  0,  0,  0,  0],\n",
      "        [ 0,  3,  4, 36,  0,  0,  0],\n",
      "        [ 0,  5, 36,  0,  0,  0,  0],\n",
      "        [ 0,  3,  6, 36,  0,  0,  0],\n",
      "        [ 0,  7,  6,  8, 36,  0,  0],\n",
      "        [ 0,  7,  4,  8, 36,  0,  0],\n",
      "        [ 0,  9, 36,  0,  0,  0,  0],\n",
      "        [ 0, 10, 36,  0,  0,  0,  0],\n",
      "        [ 0, 11, 12, 36,  0,  0,  0],\n",
      "        [ 0, 13, 36,  0,  0,  0,  0],\n",
      "        [ 0, 14,  6, 15, 36,  0,  0],\n",
      "        [ 0, 14,  4, 15, 36,  0,  0],\n",
      "        [ 0, 16, 17, 36,  0,  0,  0],\n",
      "        [ 0, 18, 19, 36,  0,  0,  0],\n",
      "        [ 0, 20, 36,  0,  0,  0,  0],\n",
      "        [ 0, 21, 22, 36,  0,  0,  0],\n",
      "        [ 0, 23, 36,  0,  0,  0,  0],\n",
      "        [ 0, 24, 25, 26,  1, 27, 36],\n",
      "        [ 0, 24, 25, 26,  5, 27, 36],\n",
      "        [ 0, 28, 29, 36,  0,  0,  0],\n",
      "        [ 0, 30, 36,  0,  0,  0,  0],\n",
      "        [ 0, 31, 36,  0,  0,  0,  0],\n",
      "        [ 0,  6,  8, 32, 36,  0,  0],\n",
      "        [ 0,  4,  8, 32, 36,  0,  0],\n",
      "        [ 0, 33, 34, 36,  0,  0,  0],\n",
      "        [ 0, 33, 35, 34, 36,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "print(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4953\n",
      "epoch: 1 total loss: 1.7374\n",
      "4953\n",
      "epoch: 2 total loss: 0.9214\n",
      "4953\n",
      "epoch: 3 total loss: 0.7106\n",
      "4953\n",
      "epoch: 4 total loss: 0.6149\n",
      "4953\n",
      "epoch: 5 total loss: 0.5310\n",
      "4953\n",
      "epoch: 6 total loss: 0.4744\n",
      "4953\n",
      "epoch: 7 total loss: 0.4536\n",
      "4953\n",
      "epoch: 8 total loss: 0.4004\n",
      "4953\n",
      "epoch: 9 total loss: 0.3873\n",
      "4953\n",
      "epoch: 10 total loss: 0.3484\n",
      "4953\n",
      "epoch: 11 total loss: 0.3269\n",
      "4953\n",
      "epoch: 12 total loss: 0.3100\n",
      "4953\n",
      "epoch: 13 total loss: 0.2977\n",
      "4953\n",
      "epoch: 14 total loss: 0.2781\n",
      "4953\n",
      "epoch: 15 total loss: 0.2688\n",
      "4953\n",
      "epoch: 16 total loss: 0.2543\n",
      "4953\n",
      "epoch: 17 total loss: 0.2457\n",
      "4953\n",
      "epoch: 18 total loss: 0.2487\n",
      "4953\n",
      "epoch: 19 total loss: 0.2401\n",
      "4953\n",
      "epoch: 20 total loss: 0.2215\n",
      "4953\n",
      "epoch: 21 total loss: 0.2255\n",
      "4953\n",
      "epoch: 22 total loss: 0.2136\n",
      "4953\n",
      "epoch: 23 total loss: 0.2105\n",
      "4953\n",
      "epoch: 24 total loss: 0.1989\n",
      "4953\n",
      "epoch: 25 total loss: 0.1993\n",
      "4953\n",
      "epoch: 26 total loss: 0.2006\n",
      "4953\n",
      "epoch: 27 total loss: 0.1934\n",
      "4953\n",
      "epoch: 28 total loss: 0.1879\n",
      "4953\n",
      "epoch: 29 total loss: 0.1857\n",
      "4953\n",
      "epoch: 30 total loss: 0.1852\n",
      "4953\n",
      "epoch: 31 total loss: 0.1861\n",
      "4953\n",
      "epoch: 32 total loss: 0.1715\n",
      "4953\n",
      "epoch: 33 total loss: 0.1762\n",
      "4953\n",
      "epoch: 34 total loss: 0.1704\n",
      "4953\n",
      "epoch: 35 total loss: 0.1802\n",
      "4953\n",
      "epoch: 36 total loss: 0.1691\n",
      "4953\n",
      "epoch: 37 total loss: 0.1636\n",
      "4953\n",
      "epoch: 38 total loss: 0.1622\n",
      "4953\n",
      "epoch: 39 total loss: 0.1617\n",
      "4953\n",
      "epoch: 40 total loss: 0.1610\n",
      "4953\n",
      "epoch: 41 total loss: 0.1612\n",
      "4953\n",
      "epoch: 42 total loss: 0.1575\n",
      "4953\n",
      "epoch: 43 total loss: 0.1526\n",
      "4953\n",
      "epoch: 44 total loss: 0.1538\n",
      "4953\n",
      "epoch: 45 total loss: 0.1484\n",
      "4953\n",
      "epoch: 46 total loss: 0.1521\n",
      "4953\n",
      "epoch: 47 total loss: 0.1549\n",
      "4953\n",
      "epoch: 48 total loss: 0.1490\n",
      "4953\n",
      "epoch: 49 total loss: 0.1408\n",
      "4953\n",
      "epoch: 50 total loss: 0.1444\n",
      "4953\n",
      "epoch: 51 total loss: 0.1540\n",
      "4953\n",
      "epoch: 52 total loss: 0.1447\n",
      "4953\n",
      "epoch: 53 total loss: 0.1440\n",
      "4953\n",
      "epoch: 54 total loss: 0.1399\n",
      "4953\n",
      "epoch: 55 total loss: 0.1476\n",
      "4953\n",
      "epoch: 56 total loss: 0.1475\n",
      "4953\n",
      "epoch: 57 total loss: 0.1418\n",
      "4953\n",
      "epoch: 58 total loss: 0.1356\n",
      "4953\n",
      "epoch: 59 total loss: 0.1408\n",
      "4953\n",
      "epoch: 60 total loss: 0.1375\n",
      "4953\n",
      "epoch: 61 total loss: 0.1359\n",
      "4953\n",
      "epoch: 62 total loss: 0.1359\n",
      "4953\n",
      "epoch: 63 total loss: 0.1410\n",
      "4953\n",
      "epoch: 64 total loss: 0.1328\n",
      "4953\n",
      "epoch: 65 total loss: 0.1350\n",
      "4953\n",
      "epoch: 66 total loss: 0.1373\n",
      "4953\n",
      "epoch: 67 total loss: 0.1357\n",
      "4953\n",
      "epoch: 68 total loss: 0.1278\n",
      "4953\n",
      "epoch: 69 total loss: 0.1299\n",
      "4953\n",
      "epoch: 70 total loss: 0.1226\n",
      "4953\n",
      "epoch: 71 total loss: 0.1348\n",
      "4953\n",
      "epoch: 72 total loss: 0.1332\n",
      "4953\n",
      "epoch: 73 total loss: 0.1307\n",
      "4953\n",
      "epoch: 74 total loss: 0.1250\n",
      "4953\n",
      "epoch: 75 total loss: 0.1246\n",
      "4953\n",
      "epoch: 76 total loss: 0.1345\n",
      "4953\n",
      "epoch: 77 total loss: 0.1222\n",
      "4953\n",
      "epoch: 78 total loss: 0.1255\n",
      "4953\n",
      "epoch: 79 total loss: 0.1244\n",
      "4953\n",
      "epoch: 80 total loss: 0.1253\n",
      "4953\n",
      "epoch: 81 total loss: 0.1282\n",
      "4953\n",
      "epoch: 82 total loss: 0.1135\n",
      "4953\n",
      "epoch: 83 total loss: 0.1334\n",
      "4953\n",
      "epoch: 84 total loss: 0.1260\n",
      "4953\n",
      "epoch: 85 total loss: 0.1216\n",
      "4953\n",
      "epoch: 86 total loss: 0.1226\n",
      "4953\n",
      "epoch: 87 total loss: 0.1305\n",
      "4953\n",
      "epoch: 88 total loss: 0.1316\n",
      "4953\n",
      "epoch: 89 total loss: 0.1235\n",
      "4953\n",
      "epoch: 90 total loss: 0.1192\n",
      "4953\n",
      "epoch: 91 total loss: 0.1231\n",
      "4953\n",
      "epoch: 92 total loss: 0.1266\n",
      "4953\n",
      "epoch: 93 total loss: 0.1205\n",
      "4953\n",
      "epoch: 94 total loss: 0.1209\n",
      "4953\n",
      "epoch: 95 total loss: 0.1188\n",
      "4953\n",
      "epoch: 96 total loss: 0.1153\n",
      "4953\n",
      "epoch: 97 total loss: 0.1223\n",
      "4953\n",
      "epoch: 98 total loss: 0.1186\n",
      "4953\n",
      "epoch: 99 total loss: 0.1165\n",
      "4953\n",
      "epoch: 100 total loss: 0.1212\n",
      "4953\n",
      "epoch: 101 total loss: 0.1152\n",
      "4953\n",
      "epoch: 102 total loss: 0.1132\n",
      "4953\n",
      "epoch: 103 total loss: 0.1178\n",
      "4953\n",
      "epoch: 104 total loss: 0.1114\n",
      "4953\n",
      "epoch: 105 total loss: 0.1145\n",
      "4953\n",
      "epoch: 106 total loss: 0.1192\n",
      "4953\n",
      "epoch: 107 total loss: 0.1160\n",
      "4953\n",
      "epoch: 108 total loss: 0.1138\n",
      "4953\n",
      "epoch: 109 total loss: 0.1139\n",
      "4953\n",
      "epoch: 110 total loss: 0.1100\n",
      "4953\n",
      "epoch: 111 total loss: 0.1121\n",
      "4953\n",
      "epoch: 112 total loss: 0.1194\n",
      "4953\n",
      "epoch: 113 total loss: 0.1184\n",
      "4953\n",
      "epoch: 114 total loss: 0.1125\n",
      "4953\n",
      "epoch: 115 total loss: 0.1110\n",
      "4953\n",
      "epoch: 116 total loss: 0.1123\n",
      "4953\n",
      "epoch: 117 total loss: 0.1099\n",
      "4953\n",
      "epoch: 118 total loss: 0.1074\n",
      "4953\n",
      "epoch: 119 total loss: 0.1081\n",
      "4953\n",
      "epoch: 120 total loss: 0.1111\n",
      "4953\n",
      "epoch: 121 total loss: 0.1079\n",
      "4953\n",
      "epoch: 122 total loss: 0.1110\n",
      "4953\n",
      "epoch: 123 total loss: 0.1088\n",
      "4953\n",
      "epoch: 124 total loss: 0.1139\n",
      "4953\n",
      "epoch: 125 total loss: 0.1032\n",
      "4953\n",
      "epoch: 126 total loss: 0.1111\n",
      "4953\n",
      "epoch: 127 total loss: 0.1081\n",
      "4953\n",
      "epoch: 128 total loss: 0.1075\n",
      "4953\n",
      "epoch: 129 total loss: 0.1080\n",
      "4953\n",
      "epoch: 130 total loss: 0.1036\n",
      "4953\n",
      "epoch: 131 total loss: 0.1099\n",
      "4953\n",
      "epoch: 132 total loss: 0.1055\n",
      "4953\n",
      "epoch: 133 total loss: 0.0985\n",
      "4953\n",
      "epoch: 134 total loss: 0.1048\n",
      "4953\n",
      "epoch: 135 total loss: 0.0969\n",
      "4953\n",
      "epoch: 136 total loss: 0.1002\n",
      "4953\n",
      "epoch: 137 total loss: 0.1002\n",
      "4953\n",
      "epoch: 138 total loss: 0.1112\n",
      "4953\n",
      "epoch: 139 total loss: 0.1059\n",
      "4953\n",
      "epoch: 140 total loss: 0.1090\n",
      "4953\n",
      "epoch: 141 total loss: 0.1103\n",
      "4953\n",
      "epoch: 142 total loss: 0.1049\n",
      "4953\n",
      "epoch: 143 total loss: 0.1048\n",
      "4953\n",
      "epoch: 144 total loss: 0.1087\n",
      "4953\n",
      "epoch: 145 total loss: 0.0974\n",
      "4953\n",
      "epoch: 146 total loss: 0.1040\n",
      "4953\n",
      "epoch: 147 total loss: 0.1072\n",
      "4953\n",
      "epoch: 148 total loss: 0.1090\n",
      "4953\n",
      "epoch: 149 total loss: 0.1055\n",
      "4953\n",
      "epoch: 150 total loss: 0.1114\n",
      "4953\n",
      "epoch: 151 total loss: 0.1030\n",
      "4953\n",
      "epoch: 152 total loss: 0.0982\n",
      "4953\n",
      "epoch: 153 total loss: 0.1006\n",
      "4953\n",
      "epoch: 154 total loss: 0.1011\n",
      "4953\n",
      "epoch: 155 total loss: 0.0973\n",
      "4953\n",
      "epoch: 156 total loss: 0.1036\n",
      "4953\n",
      "epoch: 157 total loss: 0.1060\n",
      "4953\n",
      "epoch: 158 total loss: 0.0932\n",
      "4953\n",
      "epoch: 159 total loss: 0.1028\n",
      "4953\n",
      "epoch: 160 total loss: 0.1045\n",
      "4953\n",
      "epoch: 161 total loss: 0.0991\n",
      "4953\n",
      "epoch: 162 total loss: 0.1024\n",
      "4953\n",
      "epoch: 163 total loss: 0.0982\n",
      "4953\n",
      "epoch: 164 total loss: 0.0969\n",
      "4953\n",
      "epoch: 165 total loss: 0.0974\n",
      "4953\n",
      "epoch: 166 total loss: 0.0971\n",
      "4953\n",
      "epoch: 167 total loss: 0.0989\n",
      "4953\n",
      "epoch: 168 total loss: 0.1059\n",
      "4953\n",
      "epoch: 169 total loss: 0.0979\n",
      "4953\n",
      "epoch: 170 total loss: 0.0984\n",
      "4953\n",
      "epoch: 171 total loss: 0.0967\n",
      "4953\n",
      "epoch: 172 total loss: 0.0907\n",
      "4953\n",
      "epoch: 173 total loss: 0.0989\n",
      "4953\n",
      "epoch: 174 total loss: 0.0978\n",
      "4953\n",
      "epoch: 175 total loss: 0.0964\n",
      "4953\n",
      "epoch: 176 total loss: 0.0942\n",
      "4953\n",
      "epoch: 177 total loss: 0.1007\n",
      "4953\n",
      "epoch: 178 total loss: 0.0945\n",
      "4953\n",
      "epoch: 179 total loss: 0.0906\n",
      "4953\n",
      "epoch: 180 total loss: 0.0979\n",
      "4953\n",
      "epoch: 181 total loss: 0.1017\n",
      "4953\n",
      "epoch: 182 total loss: 0.0971\n",
      "4953\n",
      "epoch: 183 total loss: 0.0993\n",
      "4953\n",
      "epoch: 184 total loss: 0.0964\n",
      "4953\n",
      "epoch: 185 total loss: 0.0939\n",
      "4953\n",
      "epoch: 186 total loss: 0.0958\n",
      "4953\n",
      "epoch: 187 total loss: 0.0982\n",
      "4953\n",
      "epoch: 188 total loss: 0.0924\n",
      "4953\n",
      "epoch: 189 total loss: 0.0909\n",
      "4953\n",
      "epoch: 190 total loss: 0.0896\n",
      "4953\n",
      "epoch: 191 total loss: 0.0920\n",
      "4953\n",
      "epoch: 192 total loss: 0.0886\n",
      "4953\n",
      "epoch: 193 total loss: 0.0914\n",
      "4953\n",
      "epoch: 194 total loss: 0.0882\n",
      "4953\n",
      "epoch: 195 total loss: 0.0929\n",
      "4953\n",
      "epoch: 196 total loss: 0.0856\n",
      "4953\n",
      "epoch: 197 total loss: 0.0926\n",
      "4953\n",
      "epoch: 198 total loss: 0.0893\n",
      "4953\n",
      "epoch: 199 total loss: 0.0946\n",
      "4953\n",
      "epoch: 200 total loss: 0.0899\n",
      "4953\n",
      "epoch: 201 total loss: 0.0909\n",
      "4953\n",
      "epoch: 202 total loss: 0.0892\n",
      "4953\n",
      "epoch: 203 total loss: 0.0962\n",
      "4953\n",
      "epoch: 204 total loss: 0.0901\n",
      "4953\n",
      "epoch: 205 total loss: 0.0987\n",
      "4953\n",
      "epoch: 206 total loss: 0.0877\n",
      "4953\n",
      "epoch: 207 total loss: 0.0848\n",
      "4953\n",
      "epoch: 208 total loss: 0.0900\n",
      "4953\n",
      "epoch: 209 total loss: 0.0931\n",
      "4953\n",
      "epoch: 210 total loss: 0.0919\n",
      "4953\n",
      "epoch: 211 total loss: 0.0861\n",
      "4953\n",
      "epoch: 212 total loss: 0.0829\n",
      "4953\n",
      "epoch: 213 total loss: 0.0912\n",
      "4953\n",
      "epoch: 214 total loss: 0.0940\n",
      "4953\n",
      "epoch: 215 total loss: 0.0903\n",
      "4953\n",
      "epoch: 216 total loss: 0.0947\n",
      "4953\n",
      "epoch: 217 total loss: 0.0886\n",
      "4953\n",
      "epoch: 218 total loss: 0.0902\n",
      "4953\n",
      "epoch: 219 total loss: 0.0943\n",
      "4953\n",
      "epoch: 220 total loss: 0.0887\n",
      "4953\n",
      "epoch: 221 total loss: 0.0850\n",
      "4953\n",
      "epoch: 222 total loss: 0.0899\n",
      "4953\n",
      "epoch: 223 total loss: 0.0894\n",
      "4953\n",
      "epoch: 224 total loss: 0.0876\n",
      "4953\n",
      "epoch: 225 total loss: 0.0873\n",
      "4953\n",
      "epoch: 226 total loss: 0.0891\n",
      "4953\n",
      "epoch: 227 total loss: 0.0799\n",
      "4953\n",
      "epoch: 228 total loss: 0.0905\n",
      "4953\n",
      "epoch: 229 total loss: 0.0894\n",
      "4953\n",
      "epoch: 230 total loss: 0.0833\n",
      "4953\n",
      "epoch: 231 total loss: 0.0838\n",
      "4953\n",
      "epoch: 232 total loss: 0.0899\n",
      "4953\n",
      "epoch: 233 total loss: 0.0852\n",
      "4953\n",
      "epoch: 234 total loss: 0.0834\n",
      "4953\n",
      "epoch: 235 total loss: 0.0927\n",
      "4953\n",
      "epoch: 236 total loss: 0.0911\n",
      "4953\n",
      "epoch: 237 total loss: 0.0896\n",
      "4953\n",
      "epoch: 238 total loss: 0.0925\n",
      "4953\n",
      "epoch: 239 total loss: 0.0938\n",
      "4953\n",
      "epoch: 240 total loss: 0.0859\n",
      "4953\n",
      "epoch: 241 total loss: 0.0836\n",
      "4953\n",
      "epoch: 242 total loss: 0.0872\n",
      "4953\n",
      "epoch: 243 total loss: 0.0862\n",
      "4953\n",
      "epoch: 244 total loss: 0.0959\n",
      "4953\n",
      "epoch: 245 total loss: 0.0860\n",
      "4953\n",
      "epoch: 246 total loss: 0.0883\n",
      "4953\n",
      "epoch: 247 total loss: 0.0925\n",
      "4953\n",
      "epoch: 248 total loss: 0.0865\n",
      "4953\n",
      "epoch: 249 total loss: 0.0894\n",
      "4953\n",
      "epoch: 250 total loss: 0.0879\n",
      "4953\n",
      "epoch: 251 total loss: 0.0844\n",
      "4953\n",
      "epoch: 252 total loss: 0.0855\n",
      "4953\n",
      "epoch: 253 total loss: 0.0893\n",
      "4953\n",
      "epoch: 254 total loss: 0.0843\n",
      "4953\n",
      "epoch: 255 total loss: 0.0899\n",
      "4953\n",
      "epoch: 256 total loss: 0.0891\n",
      "4953\n",
      "epoch: 257 total loss: 0.0846\n",
      "4953\n",
      "epoch: 258 total loss: 0.0840\n",
      "4953\n",
      "epoch: 259 total loss: 0.0856\n",
      "4953\n",
      "epoch: 260 total loss: 0.0867\n",
      "4953\n",
      "epoch: 261 total loss: 0.0861\n",
      "4953\n",
      "epoch: 262 total loss: 0.0794\n",
      "4953\n",
      "epoch: 263 total loss: 0.0812\n",
      "4953\n",
      "epoch: 264 total loss: 0.0840\n",
      "4953\n",
      "epoch: 265 total loss: 0.0843\n",
      "4953\n",
      "epoch: 266 total loss: 0.0882\n",
      "4953\n",
      "epoch: 267 total loss: 0.0853\n",
      "4953\n",
      "epoch: 268 total loss: 0.0823\n",
      "4953\n",
      "epoch: 269 total loss: 0.0871\n",
      "4953\n",
      "epoch: 270 total loss: 0.0885\n",
      "4953\n",
      "epoch: 271 total loss: 0.0805\n",
      "4953\n",
      "epoch: 272 total loss: 0.0900\n",
      "4953\n",
      "epoch: 273 total loss: 0.0814\n",
      "4953\n",
      "epoch: 274 total loss: 0.0838\n",
      "4953\n",
      "epoch: 275 total loss: 0.0832\n",
      "4953\n",
      "epoch: 276 total loss: 0.0835\n",
      "4953\n",
      "epoch: 277 total loss: 0.0818\n",
      "4953\n",
      "epoch: 278 total loss: 0.0901\n",
      "4953\n",
      "epoch: 279 total loss: 0.0834\n",
      "4953\n",
      "epoch: 280 total loss: 0.0822\n",
      "4953\n",
      "epoch: 281 total loss: 0.0804\n",
      "4953\n",
      "epoch: 282 total loss: 0.0836\n",
      "4953\n",
      "epoch: 283 total loss: 0.0860\n",
      "4953\n",
      "epoch: 284 total loss: 0.0868\n",
      "4953\n",
      "epoch: 285 total loss: 0.0894\n",
      "4953\n",
      "epoch: 286 total loss: 0.0818\n",
      "4953\n",
      "epoch: 287 total loss: 0.0825\n",
      "4953\n",
      "epoch: 288 total loss: 0.0831\n",
      "4953\n",
      "epoch: 289 total loss: 0.0840\n",
      "4953\n",
      "epoch: 290 total loss: 0.0847\n",
      "4953\n",
      "epoch: 291 total loss: 0.0758\n",
      "4953\n",
      "epoch: 292 total loss: 0.0802\n",
      "4953\n",
      "epoch: 293 total loss: 0.0825\n",
      "4953\n",
      "epoch: 294 total loss: 0.0777\n",
      "4953\n",
      "epoch: 295 total loss: 0.0911\n",
      "4953\n",
      "epoch: 296 total loss: 0.0898\n",
      "4953\n",
      "epoch: 297 total loss: 0.0845\n",
      "4953\n",
      "epoch: 298 total loss: 0.0891\n",
      "4953\n",
      "epoch: 299 total loss: 0.0849\n",
      "4953\n",
      "epoch: 300 total loss: 0.0894\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "enc.to(device)\n",
    "dec.to(device)\n",
    "tr_data.to(device)\n",
    "tr_label.to(device)\n",
    "tr_text.to(device)\n",
    "len_text.to(device)\n",
    "\n",
    "for epoch in range(300):\n",
    "    loss = trainer(\n",
    "        config, # configs\n",
    "        enc, # encoder\n",
    "        dec, # decoder\n",
    "        cross_entropy, # loss\n",
    "        optimizer, # optimizer\n",
    "        tr_data, # training input\n",
    "        tr_label, # training labels\n",
    "        tr_text, # training label text sequence\n",
    "        len_text, # training label text sequence length\n",
    "        break_step, # max training label text sequence length\n",
    "        vocab_size, # vocabulary size\n",
    "        device, # device\n",
    "    )\n",
    "\n",
    "    print(\"epoch: %d total loss: %.4f\" % (epoch + 1, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(enc.state_dict(), model_path + run_tag + '_enc.pth')\n",
    "torch.save(dec.state_dict(), model_path + run_tag + '_dec.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(37, 1024)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (decode_step): LSTMCell(1024, 128)\n",
       "  (init_h): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (init_c): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc): Linear(in_features=128, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "enc = Encoder(d_input=dim, d_model=128, d_output=128, seq_len=seq_len)\n",
    "dec = Decoder(embed_dim=1024, decoder_dim=128, vocab=word_list, encoder_dim=128, device=device)\n",
    "\n",
    "# Assuming 'config' is a dictionary containing 'run_tag'\n",
    "enc_load_path = os.path.join('model', f\"{config['run_tag']}_enc.pth\")\n",
    "dec_load_path = os.path.join('model', f\"{config['run_tag']}_dec.pth\")\n",
    "\n",
    "# Load the state dictionary from the file\n",
    "enc_state_dict = torch.load(enc_load_path)\n",
    "dec_state_dict = torch.load(dec_load_path)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "enc.load_state_dict(enc_state_dict)\n",
    "dec.load_state_dict(dec_state_dict)\n",
    "\n",
    "# Ensure the models are on the correct device\n",
    "enc.to(device)\n",
    "dec.to(device)\n",
    "#seqs = torch.load('seqs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "Test Acc: 0.9326 Macro-Prec: 0.9468 Macro-Rec: 0.9326 Macro-F1: 0.9315\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#enc.to(device)\n",
    "#dec.to(device)\n",
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "hypotheses = list()\n",
    "batch_size = test_data.size(0)\n",
    "pred_whole = torch.zeros_like(test_label)\n",
    "seqs = seqs.to(device)\n",
    "#print(seqs)\n",
    "config['batchSize'] = 16\n",
    "total_evaluation_time = 0  # Initialize total evaluation time\n",
    "total_samples = 0  # Initialize total number of samples\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inds, batch_data, batch_label, batch_text, batch_len) in enumerate(\n",
    "            DataBatch(test_data, test_label, test_text, test_len_text, config['batchSize'] , shuffle=True)\n",
    "        ):\n",
    "            #pred_whole = torch.zeros_like(test_label)\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_label = batch_label.to(device)\n",
    "            batch_text = batch_text.to(device)\n",
    "            batch_len = batch_len.to(device)\n",
    "            #print( batch_idx )\n",
    "            #print( inds)\n",
    "            \n",
    "\n",
    "            batch_size = batch_data.size(0)\n",
    "            \n",
    "            total_samples += batch_size  # Accumulate the number of samples\n",
    "            start_time = time.time()\n",
    "            #print(enc.layer1.weight.device)\n",
    "            #print(batch_data.device)\n",
    "            #print(dec.init_h.weight.device)\n",
    "            encoder_out = enc(batch_data)  # (batch_size, enc_seq_len, encoder_dim)\n",
    "            #print(encoder_out.shape)\n",
    "            enc_seq_len = encoder_out.size(1)\n",
    "            encoder_dim = encoder_out.size(2)\n",
    "\n",
    "            encoder_out = encoder_out.unsqueeze(1).expand(batch_size, class_num, enc_seq_len, encoder_dim)\n",
    "            encoder_out = encoder_out.reshape(batch_size * class_num, enc_seq_len, encoder_dim)\n",
    "            #print(seqs)\n",
    "            k_prev_words = seqs[:, 0].unsqueeze(0).expand(batch_size, class_num).long()  # (batch_size, class_num)\n",
    "            #print(k_prev_words)\n",
    "            k_prev_words = k_prev_words.reshape(batch_size * class_num, 1)  # (batch_size * class_num, 1)\n",
    "            \n",
    "            h, c = dec.init_hidden_state(encoder_out)\n",
    "\n",
    "            seq_scores = torch.zeros((batch_size, class_num)).to(device)\n",
    "\n",
    "            for step in range(1, break_step):\n",
    "                embeddings = dec.embedding(k_prev_words).squeeze(1)  # (batch_size * class_num, embed_dim)\n",
    "                h, c = dec.decode_step(embeddings, (h, c))\n",
    "                scores = dec.fc(h.reshape(batch_size, class_num, -1))  # (batch_size, class_num, vocab_size)\n",
    "                scores = F.log_softmax(scores, dim=-1)\n",
    "                k_prev_words = seqs[:, step].unsqueeze(0).expand(batch_size, class_num).long()\n",
    "                for batch_i in range(batch_size):\n",
    "                    for class_i in range(class_num):\n",
    "                        if k_prev_words[batch_i, class_i] != 0:\n",
    "                            seq_scores[batch_i, class_i] += scores[batch_i, class_i, k_prev_words[batch_i, class_i]]\n",
    "                k_prev_words = k_prev_words.reshape(batch_size * class_num, 1)  # (batch_size * class_num, 1)\n",
    "            #print( seq_scores )\n",
    "            max_indices = seq_scores.argmax(dim=1)\n",
    "            for batch_i in range(batch_size):\n",
    "                max_i = max_indices[batch_i]\n",
    "                seq = seqs[max_i].tolist()\n",
    "                hypotheses.append([w for w in seq if w not in {0, vocab_size - 1}])\n",
    "                print(batch_i + batch_idx * config['batchSize'])\n",
    "                #pred_whole[batch_i + batch_idx * config['batchSize']] = pred_dict[\"#\".join(map(str, hypotheses[-1]))]\n",
    "                #print(batch_i + batch_idx * config['batchSize'])\n",
    "                #print\n",
    "                pred_whole[inds[batch_i]] = pred_dict[\"#\".join(map(str, hypotheses[-1]))]\n",
    "                #print(test_label[inds[batch_i]])\n",
    "                #print( pred_whole[inds[batch_i]] )\n",
    "            #print( pred_whole.shape)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            batch_evaluation_time = end_time - start_time  # Calculate batch evaluation time\n",
    "            total_evaluation_time += batch_evaluation_time  # Accumulate total evaluation time\n",
    "\n",
    "            #acc = accuracy_score(test_label.cpu().numpy(), pred_whole.cpu().numpy())\n",
    "            #prec = precision_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
    "            #rec = recall_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
    "            #f1 = f1_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
    "\n",
    "            #print(f'Total Evaluation Time: {total_evaluation_time:.2f} seconds')\n",
    "    \n",
    "            # Calculate evaluation time per batch and per sample\n",
    "            #eval_time_per_batch = total_evaluation_time / (batch_idx + 1)\n",
    "            #eval_time_per_sample = total_evaluation_time / total_samples\n",
    "\n",
    "            #print(f'Average Evaluation Time per Batch: {eval_time_per_batch:.2f} seconds')\n",
    "            #print(f'Average Evaluation Time per Sample: {eval_time_per_sample:.6f} seconds')\n",
    "            #print('Test Acc: %.4f Macro-Prec: %.4f Macro-Rec: %.4f Macro-F1: %.4f' % (acc, prec, rec, f1))\n",
    "\n",
    "            #print(f'Batch {batch_idx + 1} Evaluation Time: {batch_evaluation_time:.2f} seconds')\n",
    "#print( pred_whole.shape )\n",
    "#print( test_label.shape )\n",
    "\n",
    "acc = accuracy_score(test_label.cpu().numpy(), pred_whole.cpu().numpy())\n",
    "prec = precision_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
    "rec = recall_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
    "f1 = f1_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
    "\n",
    "#print(f'Total Evaluation Time: {total_evaluation_time:.2f} seconds')\n",
    "    \n",
    "    # Calculate evaluation time per batch and per sample\n",
    "#eval_time_per_batch = total_evaluation_time / (batch_idx + 1)\n",
    "#eval_time_per_sample = total_evaluation_time / total_samples\n",
    "\n",
    "#print(f'Average Evaluation Time per Batch: {eval_time_per_batch:.2f} seconds')\n",
    "#print(f'Average Evaluation Time per Sample: {eval_time_per_sample:.6f} seconds')\n",
    "print('Test Acc: %.4f Macro-Prec: %.4f Macro-Rec: %.4f Macro-F1: %.4f' % (acc, prec, rec, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
